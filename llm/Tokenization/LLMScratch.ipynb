{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a570234",
   "metadata": {},
   "source": [
    "Step 1 Tokenizing the Short Story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd931085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of characters: 20480\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no g\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    verdict=f.read()\n",
    "\n",
    "print(\"total number of characters:\",len(verdict))\n",
    "print(verdict[:100])  # Print the first 100 characters for a preview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cd1e31",
   "metadata": {},
   "source": [
    "Our goal is to tokenize all the word.\n",
    "\n",
    "Q How can we best split the text into individual tokens?\n",
    "ans- Use Regular expression Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d6041f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world!', ' ', 'This', ' ', 'is', ' ', 'a', ' ', 'test', '.', '', ' ', \"Let's\", ' ', 'see', ' ', 'how', ' ', 'many', ' ', 'sentences', ' ', 'we', ' ', 'can', ' ', 'count', '.', '', ' ', 'Can', ' ', 'you', ' ', 'count', ' ', 'them', ' ', 'all', '?', '', ' ', 'I', ' ', 'hope', ' ', 'so', '.', '']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text=\"Hello, world! This is a test. Let's see how many sentences we can count. Can you count them all? I hope so.\"\n",
    "\n",
    "result=re.split(r'([,.?]|\\s)',text)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "529be715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world!', 'This', 'is', 'a', 'test', '.', \"Let's\", 'see', 'how', 'many', 'sentences', 'we', 'can', 'count', '.', 'Can', 'you', 'count', 'them', 'all', '?', 'I', 'hope', 'so', '.']\n"
     ]
    }
   ],
   "source": [
    "result=[item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15667134",
   "metadata": {},
   "source": [
    "Removing whitespaces or not\n",
    "when developing simple tokenizer we don't need white space but if we want to build tokenizer of python code where indentation is mandatory we need white space . it depend on use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5315b116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '!', '', ' ', 'This', ' ', 'is', ' ', 'a', ' ', 'test', '.', '', ' ', \"Let's\", ' ', 'see', ' ', 'how', ' ', 'many', ' ', 'sentences', ' ', 'we', ' ', 'can', ' ', 'count', '.', '', ' ', 'Can', ' ', 'you', ' ', 'count', ' ', 'them', ' ', 'all', '?', '', ' ', 'I', ' ', 'hope', ' ', 'so', '.', '']\n",
      "['Hello', ',', 'world', '!', 'This', 'is', 'a', 'test', '.', \"Let's\", 'see', 'how', 'many', 'sentences', 'we', 'can', 'count', '.', 'Can', 'you', 'count', 'them', 'all', '?', 'I', 'hope', 'so', '.']\n"
     ]
    }
   ],
   "source": [
    "result=re.split(r'([,.:;!?_\"()\\--]|\\s)',text)\n",
    "print(result)\n",
    "result=[item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc44f193",
   "metadata": {},
   "source": [
    "Now that we got a basic tokenizer working , let's apply it to verdict text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "baa44ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ', ', 'in', 'the', 'height', 'of', 'his', 'glory', ', ', 'he', 'had', 'dropped', 'his', 'painting', ', ', 'married', 'a', 'rich', 'widow', ', ', 'and', 'established', 'himself', 'in', 'a', 'villa', 'on', 'the', 'Riviera', '. (', 'Though', 'I', 'rather', 'thought', 'it', 'would', 'have', 'been', 'Rome', 'or', 'Florence', '.)\\n\\n', '\"The', 'height', 'of', 'his', 'glory\"', '--', 'that', 'was', 'what', 'the', 'women', 'called', 'it', '. ', 'I', 'can', 'hear', 'Mrs', '. ', 'Gideon', 'Thwing', '--', 'his', 'last', 'Chicago', 'sitter', '--', 'deploring', 'his', 'unaccountable', 'abdication']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "preprocessed_text = re.split(r'(--|[,.?;:!_()\\]\\s]+)', verdict)\n",
    "preprocessed_text = [item for item in preprocessed_text if item.strip()]\n",
    "print(preprocessed_text[:100])  # Print the first 100 items for a preview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffc85bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4411\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocessed_text))  # Print the total number of items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18dbb43",
   "metadata": {},
   "source": [
    "Step 2 Convert Tokens into Token IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d860d77d",
   "metadata": {},
   "source": [
    "Now we will take all the unique tokens and sort them alphabetically to determine the vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c35df46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1198\n"
     ]
    }
   ],
   "source": [
    "all_words=sorted(set(preprocessed_text))\n",
    "vocab_size=len(all_words)\n",
    "print(\"Vocabulary size:\",vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1931e942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# print(all_words[:50])\n",
    "print(type(all_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d498ad",
   "metadata": {},
   "source": [
    "Lets map word to ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99daa864",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab={token:integer for integer,token in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630740a0",
   "metadata": {},
   "source": [
    "This line creates a dictionary called `vocab` that maps each unique token in the list `all_words` to a unique integer. The `enumerate(all_words)` function returns pairs of an integer index and the corresponding token from `all_words`. The dictionary comprehension `{token:integer for integer,token in enumerate(all_words)}` then constructs key-value pairs where each token is a key and its assigned integer is the value.\n",
    "\n",
    "This mapping is commonly used in natural language processing tasks to convert words or tokens into numerical representations, which are easier for machine learning models to process. By assigning each token a unique integer, you can efficiently encode text data for further analysis or model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49eabf70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e1df05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: (' (', 0)\n",
      "1: (' _', 1)\n",
      "2: ('!', 2)\n",
      "3: ('!\\n\\n', 3)\n",
      "4: ('! ', 4)\n",
      "5: ('\"', 5)\n",
      "6: ('\"Ah', 6)\n",
      "7: ('\"Be', 7)\n",
      "8: ('\"Begin', 8)\n",
      "9: ('\"By', 9)\n",
      "10: ('\"Come', 10)\n",
      "11: ('\"Destroyed', 11)\n",
      "12: ('\"Don\\'t', 12)\n",
      "13: ('\"Gisburns\"', 13)\n",
      "14: ('\"Grindles', 14)\n",
      "15: ('\"Hang', 15)\n",
      "16: ('\"Has', 16)\n",
      "17: ('\"How', 17)\n",
      "18: ('\"I', 18)\n",
      "19: ('\"I\\'d', 19)\n",
      "20: ('\"If', 20)\n",
      "21: ('\"It', 21)\n",
      "22: ('\"It\\'s', 22)\n",
      "23: ('\"Jack', 23)\n",
      "24: ('\"Money\\'s', 24)\n",
      "25: ('\"Moon-dancers\"', 25)\n",
      "26: ('\"Mr', 26)\n",
      "27: ('\"Mrs', 27)\n",
      "28: ('\"My', 28)\n",
      "29: ('\"Never', 29)\n",
      "30: ('\"Of', 30)\n",
      "31: ('\"Oh', 31)\n",
      "32: ('\"Once', 32)\n",
      "33: ('\"Only', 33)\n",
      "34: ('\"Or', 34)\n",
      "35: ('\"That', 35)\n",
      "36: ('\"The', 36)\n",
      "37: ('\"Then', 37)\n",
      "38: ('\"There', 38)\n",
      "39: ('\"This', 39)\n",
      "40: ('\"We', 40)\n",
      "41: ('\"Well', 41)\n",
      "42: ('\"What', 42)\n",
      "43: ('\"When', 43)\n",
      "44: ('\"Why', 44)\n",
      "45: ('\"Yes', 45)\n",
      "46: ('\"You', 46)\n",
      "47: ('\"but', 47)\n",
      "48: ('\"deadening', 48)\n",
      "49: ('\"dragged', 49)\n",
      "50: ('\"effects\"', 50)\n",
      "51: ('\"interesting\"', 51)\n"
     ]
    }
   ],
   "source": [
    "for key,value in enumerate(vocab.items()):\n",
    "    print(f\"{key}: {value}\")\n",
    "    if key>50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcbb502",
   "metadata": {},
   "source": [
    "This code iterates over the items in the [`vocab`]LLMScratch.ipynb ) dictionary using the [`enumerate()`]LLMScratch.ipynb ) function. The [`vocab.items()`]LLMScratch.ipynb ) method returns each key-value pair from the dictionary as a tuple, where the key is typically a token (such as a word) and the value is its corresponding integer index.\n",
    "\n",
    "By wrapping [`vocab.items()`]LLMScratch.ipynb ) with [`enumerate()`]LLMScratch.ipynb ), each iteration provides a running integer counter (`key`) starting from 0, and the actual dictionary item (`value`). Inside the loop, the code prints the counter and the dictionary item in a formatted string. The `if key>50: break` statement ensures that the loop stops after printing the first 51 items, which is useful for previewing a sample of the vocabulary without overwhelming the output.\n",
    "\n",
    "This approach is commonly used to inspect a subset of large dictionaries, such as vocabularies in natural language processing tasks, to verify their contents or debug the token-to-index mapping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ede1f6",
   "metadata": {},
   "source": [
    "### Let's create Tokenization class and implement two methods\n",
    " 1. encoder \n",
    " 2. decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d06ad46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed= re.split(r'(--|[,.?;:!_()\\]\\s]+)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids=[self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text=\" \".join([self.int_to_str[i] for i in ids])\n",
    "    # Replace spaces before the specified punctuation\n",
    "        text = re.sub(r'\\s([,.?;:!_()\\]])', r'\\1', text)\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a670c712",
   "metadata": {},
   "source": [
    "The `SimpleTokenizerV1` class provides a basic way to convert text into a sequence of integers (tokenization) and back (detokenization) using a given vocabulary mapping. When an instance is created, it receives a `vocab` dictionary mapping string tokens to unique integers. The constructor (`__init__`) stores this mapping as `str_to_int` and also creates a reverse mapping, `int_to_str`, to convert integers back to their corresponding string tokens.\n",
    "\n",
    "The `encode` method takes a text string and splits it into tokens using a regular expression that separates words and punctuation. It then strips whitespace from each token and filters out any empty strings. Each token is then mapped to its integer ID using the vocabulary, resulting in a list of integers representing the input text.\n",
    "\n",
    "The `decode` method performs the reverse operation. It takes a list of integer IDs, converts each back to its string token, and joins them into a single string with spaces. To improve readability, it uses a regular expression to remove spaces that appear before certain punctuation marks, ensuring the reconstructed text looks natural.\n",
    "\n",
    "Overall, this class demonstrates a simple approach to text tokenization and detokenization, which is a foundational step in many natural language processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94428722",
   "metadata": {},
   "source": [
    "## -------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20a9661a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57, 31, 66, 116, 664, 612, 66, 223, 597, 664, 732, 68, 799, 653, 590, 203, 597, 1141, 382, 69, 5, 116, 434, 766, 1135, 644, 69, 43, 942, 935, 519, 1192, 79, 5, 45, 68, 881, 642, 1080, 1050, 649, 69, 149, 1139, 612, 1131, 68, 223, 307, 732, 2, 5]\n"
     ]
    }
   ],
   "source": [
    "tokenizer=SimpleTokenizerV1(vocab)\n",
    "text='''' \"Oh, I knew him, and he knew me--only it happened after he was dead.\"\n",
    "\n",
    "I dropped my voice instinctively. \"When she sent for you?\"\n",
    "\n",
    "\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\" '''\n",
    "\n",
    "ids=tokenizer.encode(text)\n",
    "print(ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e450144d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' \"Oh, I knew him, and he knew me -- only it happened after he was dead. \" I dropped my voice instinctively. \"When she sent for you? \" \"Yes -- quite insensible to the irony. She wanted him vindicated -- and by me! \"\n"
     ]
    }
   ],
   "source": [
    "text2=tokenizer.decode(ids)\n",
    "print(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e797efc",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Hello'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m text3=\u001b[33m'''\u001b[39m\u001b[33m Hello do you like tea? I like tea, but I also like coffee. Do you like coffee? Yes, I do!\u001b[39m\u001b[33m'''\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m ids2=\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext3\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(ids2)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mSimpleTokenizerV1.encode\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m      7\u001b[39m preprocessed= re.split(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m(--|[,.?;:!_()\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m]\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms]+)\u001b[39m\u001b[33m'\u001b[39m, text)\n\u001b[32m      8\u001b[39m preprocessed = [item.strip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item.strip()]\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m ids=\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstr_to_int\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpreprocessed\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m      7\u001b[39m preprocessed= re.split(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m(--|[,.?;:!_()\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m]\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms]+)\u001b[39m\u001b[33m'\u001b[39m, text)\n\u001b[32m      8\u001b[39m preprocessed = [item.strip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item.strip()]\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m ids=[\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstr_to_int\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[31mKeyError\u001b[39m: 'Hello'"
     ]
    }
   ],
   "source": [
    "text3=''' Hello do you like tea? I like tea, but I also like coffee. Do you like coffee? Yes, I do!'''\n",
    "ids2=tokenizer.encode(text3)\n",
    "print(ids2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876cddf4",
   "metadata": {},
   "source": [
    "KeyError:'Hello', occured due lack of training dataset( so less tokens. )\n",
    "So to solve this error we huge number of data to train .\n",
    "But we can solve with\n",
    " ### special context tokens. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f943cd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
