{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a570234",
   "metadata": {},
   "source": [
    "Step 1 Tokenizing the Short Story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd931085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of characters: 20480\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no g\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    verdict=f.read()\n",
    "\n",
    "print(\"total number of characters:\",len(verdict))\n",
    "print(verdict[:100])  # Print the first 100 characters for a preview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cd1e31",
   "metadata": {},
   "source": [
    "Our goal is to tokenize all the word.\n",
    "\n",
    "Q How can we best split the text into individual tokens?\n",
    "ans- Use Regular expression Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d6041f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world!', ' ', 'This', ' ', 'is', ' ', 'a', ' ', 'test', '.', '', ' ', \"Let's\", ' ', 'see', ' ', 'how', ' ', 'many', ' ', 'sentences', ' ', 'we', ' ', 'can', ' ', 'count', '.', '', ' ', 'Can', ' ', 'you', ' ', 'count', ' ', 'them', ' ', 'all', '?', '', ' ', 'I', ' ', 'hope', ' ', 'so', '.', '']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text=\"Hello, world! This is a test. Let's see how many sentences we can count. Can you count them all? I hope so.\"\n",
    "\n",
    "result=re.split(r'([,.?]|\\s)',text)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "529be715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world!', 'This', 'is', 'a', 'test', '.', \"Let's\", 'see', 'how', 'many', 'sentences', 'we', 'can', 'count', '.', 'Can', 'you', 'count', 'them', 'all', '?', 'I', 'hope', 'so', '.']\n"
     ]
    }
   ],
   "source": [
    "result=[item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15667134",
   "metadata": {},
   "source": [
    "Removing whitespaces or not\n",
    "when developing simple tokenizer we don't need white space but if we want to build tokenizer of python code where indentation is mandatory we need white space . it depend on use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5315b116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '!', '', ' ', 'This', ' ', 'is', ' ', 'a', ' ', 'test', '.', '', ' ', \"Let's\", ' ', 'see', ' ', 'how', ' ', 'many', ' ', 'sentences', ' ', 'we', ' ', 'can', ' ', 'count', '.', '', ' ', 'Can', ' ', 'you', ' ', 'count', ' ', 'them', ' ', 'all', '?', '', ' ', 'I', ' ', 'hope', ' ', 'so', '.', '']\n",
      "['Hello', ',', 'world', '!', 'This', 'is', 'a', 'test', '.', \"Let's\", 'see', 'how', 'many', 'sentences', 'we', 'can', 'count', '.', 'Can', 'you', 'count', 'them', 'all', '?', 'I', 'hope', 'so', '.']\n"
     ]
    }
   ],
   "source": [
    "result=re.split(r'([,.:;!?_\"()\\--]|\\s)',text)\n",
    "print(result)\n",
    "result=[item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc44f193",
   "metadata": {},
   "source": [
    "Now that we got a basic tokenizer working , let's apply it to verdict text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baa44ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ', ', 'in', 'the', 'height', 'of', 'his', 'glory', ', ', 'he', 'had', 'dropped', 'his', 'painting', ', ', 'married', 'a', 'rich', 'widow', ', ', 'and', 'established', 'himself', 'in', 'a', 'villa', 'on', 'the', 'Riviera', '. (', 'Though', 'I', 'rather', 'thought', 'it', 'would', 'have', 'been', 'Rome', 'or', 'Florence', '.)\\n\\n', '\"The', 'height', 'of', 'his', 'glory\"', '--', 'that', 'was', 'what', 'the', 'women', 'called', 'it', '. ', 'I', 'can', 'hear', 'Mrs', '. ', 'Gideon', 'Thwing', '--', 'his', 'last', 'Chicago', 'sitter', '--', 'deploring', 'his', 'unaccountable', 'abdication']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "preprocessed_text = re.split(r'(--|[,.?;:!_()\\]\\s]+)', verdict)\n",
    "preprocessed_text = [item for item in preprocessed_text if item.strip()]\n",
    "print(preprocessed_text[:100])  # Print the first 100 items for a preview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffc85bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4411\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocessed_text))  # Print the total number of items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18dbb43",
   "metadata": {},
   "source": [
    "Step 2 Convert Tokens into Token IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d860d77d",
   "metadata": {},
   "source": [
    "Now we will take all the unique tokens and sort them alphabetically to determine the vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c35df46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1198\n"
     ]
    }
   ],
   "source": [
    "all_words=sorted(set(preprocessed_text))\n",
    "vocab_size=len(all_words)\n",
    "print(\"Vocabulary size:\",vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d498ad",
   "metadata": {},
   "source": [
    "Lets map word to ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99daa864",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab={token:integer for integer,token in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630740a0",
   "metadata": {},
   "source": [
    "This line creates a dictionary called `vocab` that maps each unique token in the list `all_words` to a unique integer. The `enumerate(all_words)` function returns pairs of an integer index and the corresponding token from `all_words`. The dictionary comprehension `{token:integer for integer,token in enumerate(all_words)}` then constructs key-value pairs where each token is a key and its assigned integer is the value.\n",
    "\n",
    "This mapping is commonly used in natural language processing tasks to convert words or tokens into numerical representations, which are easier for machine learning models to process. By assigning each token a unique integer, you can efficiently encode text data for further analysis or model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49eabf70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e1df05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: (' (', 0)\n",
      "1: (' _', 1)\n",
      "2: ('!', 2)\n",
      "3: ('!\\n\\n', 3)\n",
      "4: ('! ', 4)\n",
      "5: ('\"', 5)\n",
      "6: ('\"Ah', 6)\n",
      "7: ('\"Be', 7)\n",
      "8: ('\"Begin', 8)\n",
      "9: ('\"By', 9)\n",
      "10: ('\"Come', 10)\n",
      "11: ('\"Destroyed', 11)\n",
      "12: ('\"Don\\'t', 12)\n",
      "13: ('\"Gisburns\"', 13)\n",
      "14: ('\"Grindles', 14)\n",
      "15: ('\"Hang', 15)\n",
      "16: ('\"Has', 16)\n",
      "17: ('\"How', 17)\n",
      "18: ('\"I', 18)\n",
      "19: ('\"I\\'d', 19)\n",
      "20: ('\"If', 20)\n",
      "21: ('\"It', 21)\n",
      "22: ('\"It\\'s', 22)\n",
      "23: ('\"Jack', 23)\n",
      "24: ('\"Money\\'s', 24)\n",
      "25: ('\"Moon-dancers\"', 25)\n",
      "26: ('\"Mr', 26)\n",
      "27: ('\"Mrs', 27)\n",
      "28: ('\"My', 28)\n",
      "29: ('\"Never', 29)\n",
      "30: ('\"Of', 30)\n",
      "31: ('\"Oh', 31)\n",
      "32: ('\"Once', 32)\n",
      "33: ('\"Only', 33)\n",
      "34: ('\"Or', 34)\n",
      "35: ('\"That', 35)\n",
      "36: ('\"The', 36)\n",
      "37: ('\"Then', 37)\n",
      "38: ('\"There', 38)\n",
      "39: ('\"This', 39)\n",
      "40: ('\"We', 40)\n",
      "41: ('\"Well', 41)\n",
      "42: ('\"What', 42)\n",
      "43: ('\"When', 43)\n",
      "44: ('\"Why', 44)\n",
      "45: ('\"Yes', 45)\n",
      "46: ('\"You', 46)\n",
      "47: ('\"but', 47)\n",
      "48: ('\"deadening', 48)\n",
      "49: ('\"dragged', 49)\n",
      "50: ('\"effects\"', 50)\n",
      "51: ('\"interesting\"', 51)\n"
     ]
    }
   ],
   "source": [
    "for key,value in enumerate(vocab.items()):\n",
    "    print(f\"{key}: {value}\")\n",
    "    if key>50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcbb502",
   "metadata": {},
   "source": [
    "This code iterates over the items in the [`vocab`]LLMScratch.ipynb ) dictionary using the [`enumerate()`]LLMScratch.ipynb ) function. The [`vocab.items()`]LLMScratch.ipynb ) method returns each key-value pair from the dictionary as a tuple, where the key is typically a token (such as a word) and the value is its corresponding integer index.\n",
    "\n",
    "By wrapping [`vocab.items()`]LLMScratch.ipynb ) with [`enumerate()`]LLMScratch.ipynb ), each iteration provides a running integer counter (`key`) starting from 0, and the actual dictionary item (`value`). Inside the loop, the code prints the counter and the dictionary item in a formatted string. The `if key>50: break` statement ensures that the loop stops after printing the first 51 items, which is useful for previewing a sample of the vocabulary without overwhelming the output.\n",
    "\n",
    "This approach is commonly used to inspect a subset of large dictionaries, such as vocabularies in natural language processing tasks, to verify their contents or debug the token-to-index mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdc6441",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
