{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26c32b38",
   "metadata": {},
   "source": [
    "# Data Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bccbdb",
   "metadata": {},
   "source": [
    "## Today's Agenda\n",
    "\n",
    "1. What is feature extraction from text/image?  \n",
    "2. Why do we need it?\n",
    "3. Why is it so difficult?\n",
    "4. What is the core idea?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda2a455",
   "metadata": {},
   "source": [
    "Here is your agenda in plain text:\n",
    "\n",
    "## Today's Agenda\n",
    "\n",
    "1. What is feature extraction from text/image?  \n",
    "Feature extraction is the process of transforming raw data (like text or images) into numerical features that can be used for machine learning. For text, this could mean converting words into vectors (such as using Bag of Words, TF-IDF, or word embeddings). For images, it could involve extracting edges, shapes, or using deep learning to get feature vectors.\n",
    "\n",
    "2. Why do we need it?  \n",
    "Machine learning algorithms require numerical input. Raw text or images cannot be directly used by most algorithms, so feature extraction converts them into a format that models can understand and learn from.\n",
    "\n",
    "3. Why is it so difficult?  \n",
    "Raw data is often complex, high-dimensional, and unstructured. Capturing the most important information while ignoring noise is challenging. Also, different tasks may require different features, and the best features are not always obvious.\n",
    "\n",
    "4. What is the core idea?  \n",
    "The core idea is to represent complex data in a simplified, structured, and informative way that preserves the essential information needed for the task (such as classification or clustering), while reducing noise and irrelevant details.\n",
    "\n",
    "5. Some techniques  \n",
    "- For text: Bag of Words, TF-IDF, word embeddings (Word2Vec, GloVe, BERT)\n",
    "- For images: SIFT, HOG, color histograms, CNN feature maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30aef417",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ef726c",
   "metadata": {},
   "source": [
    "**TF-IDF** stands for **Term Frequency–Inverse Document Frequency**. It is a statistical measure used to evaluate how important a word is to a document in a collection (corpus).\n",
    "\n",
    "### How it works:\n",
    "- **Term Frequency (TF):**  \n",
    "  Measures how frequently a word appears in a document.  \n",
    "  TF = (Number of times the word appears in the document) / (Total words in the document)\n",
    "\n",
    "- **Inverse Document Frequency (IDF):**  \n",
    "  Measures how important a word is across all documents. Rare words get a higher score.  \n",
    "  IDF = log(Total number of documents / Number of documents containing the word)\n",
    "\n",
    "- **TF-IDF Score:**  \n",
    "  TF-IDF = TF × IDF  \n",
    "  A high TF-IDF score means the word is frequent in a document but rare in the corpus, making it important for that document.\n",
    "\n",
    "### Why use TF-IDF?\n",
    "- It helps highlight unique words in each document.\n",
    "- Common words (like \"the\", \"is\") get lower scores, while rare, meaningful words get higher scores.\n",
    "- Widely used in text mining, search engines, and document classification.\n",
    "\n",
    "**Example:**  \n",
    "If \"pizza\" appears often in one review but rarely in others, it will have a high TF-IDF score for that review, indicating its importance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18554586",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220a32ff",
   "metadata": {},
   "source": [
    "### TF-IDF Example with Calculation\n",
    "\n",
    "Suppose you have the following 3 documents:\n",
    "\n",
    "- **Document 1:** \"I love pizza\"\n",
    "- **Document 2:** \"I love pasta\"\n",
    "- **Document 3:** \"Pizza and pasta are delicious\"\n",
    "\n",
    "Let's calculate the TF-IDF for the word **\"pizza\"** in **Document 1**.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. **Term Frequency (TF)**\n",
    "TF = (Number of times \"pizza\" appears in Document 1) / (Total words in Document 1)  \n",
    "TF = 1 / 3 = **0.333**\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Inverse Document Frequency (IDF)**\n",
    "First, count in how many documents \"pizza\" appears:\n",
    "- \"pizza\" appears in Document 1 and Document 3 → **2 documents**\n",
    "\n",
    "IDF = log(Total number of documents / Number of documents containing \"pizza\")  \n",
    "IDF = log(3 / 2) ≈ **0.176**\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **TF-IDF Score**\n",
    "TF-IDF = TF × IDF  \n",
    "TF-IDF = 0.333 × 0.176 ≈ **0.059**\n",
    "\n",
    "---\n",
    "\n",
    "**Interpretation:**  \n",
    "- The TF-IDF score for \"pizza\" in Document 1 is **0.059**.\n",
    "- This means \"pizza\" is somewhat important in Document 1, but since it appears in more than one document, its uniqueness is reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "796c9799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        and       are  delicious      love     pasta     pizza\n",
      "0  0.000000  0.000000   0.000000  0.707107  0.000000  0.707107\n",
      "1  0.000000  0.000000   0.000000  0.707107  0.707107  0.000000\n",
      "2  0.490479  0.490479   0.490479  0.000000  0.373022  0.373022\n"
     ]
    }
   ],
   "source": [
    "# Example: Calculating TF-IDF using scikit-learn\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"I love pizza\",\n",
    "    \"I love pasta\",\n",
    "    \"Pizza and pasta are delicious\"\n",
    "]\n",
    "\n",
    "# Create the vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the documents\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get feature names (words)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert to dense matrix and print\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bd927e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['i', 'love', 'pizza', 'and', 'pasta', '!']\n",
      "Token IDs: [101, 1045, 2293, 10733, 1998, 24857, 999, 102]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Choose a model (e.g., 'bert-base-uncased' or 'gpt2')\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "text = \"I love pizza and pasta!\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "token_ids = tokenizer.encode(text)\n",
    "\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Token IDs:\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327a2cbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe1ca28d",
   "metadata": {},
   "source": [
    "**Bag of Words (BoW)** is a simple and popular technique for representing text data in machine learning.\n",
    "\n",
    "- In BoW, each document is represented as a vector of word counts (or frequencies), ignoring grammar and word order.\n",
    "- The vocabulary is built from all unique words in the corpus.\n",
    "- Each position in the vector corresponds to a word from the vocabulary, and the value is the number of times that word appears in the document.\n",
    "\n",
    "**Example:**  \n",
    "Suppose your corpus has these two sentences:  \n",
    "1. \"I love pizza\"  \n",
    "2. \"I love pasta\"\n",
    "\n",
    "The vocabulary is: `[\"I\", \"love\", \"pizza\", \"pasta\"]`\n",
    "\n",
    "The BoW vectors are:  \n",
    "- \"I love pizza\" → [1, 1, 1, 0]  \n",
    "- \"I love pasta\" → [1, 1, 0, 1]\n",
    "\n",
    "BoW is widely used for text classification and information retrieval, but it does not capture word order or context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaeb5b5",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d051dbe",
   "metadata": {},
   "source": [
    "**One hot encoding** is a technique used to convert categorical data (such as words or labels) into a numerical format that can be used by machine learning algorithms.\n",
    "\n",
    "In one hot encoding:\n",
    "- Each unique category or word is represented as a binary vector.\n",
    "- The vector has the same length as the number of unique categories.\n",
    "- Only the position corresponding to the category is set to 1; all other positions are 0.\n",
    "\n",
    "**Example:**  \n",
    "Suppose you have three categories: `cat`, `dog`, `mouse`.\n",
    "\n",
    "| Category | One Hot Encoding |\n",
    "|----------|------------------|\n",
    "| cat      | [1, 0, 0]        |\n",
    "| dog      | [0, 1, 0]        |\n",
    "| mouse    | [0, 0, 1]        |\n",
    "\n",
    "This method is commonly used for representing categorical variables in machine learning and for representing words in text data before more advanced techniques like embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6ba681",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ac1b9b",
   "metadata": {},
   "source": [
    "**Drawbacks of One Hot Encoding:**\n",
    "\n",
    "- High Dimensionality: For large vocabularies or many categories, the resulting vectors become very large and sparse, consuming a lot of memory.\n",
    "- No Semantic Meaning: It does not capture any relationship or similarity between categories or words (for example, \"cat\" and \"dog\" are as different as \"cat\" and \"car\").\n",
    "- Curse of Dimensionality: High-dimensional data can make machine learning models less efficient and harder to train.\n",
    "- Not Suitable for Rare Categories: Categories that appear rarely may not contribute much and can be ignored by the model.\n",
    "____\n",
    "**Drawbacks of Bag of Words:**\n",
    "\n",
    "- Ignores Word Order: Bag of Words does not consider the order of words, so important context or meaning can be lost.\n",
    "- High Dimensionality: Like one hot encoding, Bag of Words creates large, sparse vectors for big vocabularies.\n",
    "- No Semantic Information: It treats all words as independent and does not capture word meanings or relationships.\n",
    "- Sensitive to Vocabulary Size: Adding new words to the corpus changes the vector size, making it hard to handle new or unseen words.\n",
    "- Cannot Handle Synonyms: Different words with similar meanings are treated as completely different features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1c740c",
   "metadata": {},
   "source": [
    "____\n",
    "For sentiment analysis, you can use several techniques depending on your data size, accuracy needs, and resources:\n",
    "\n",
    "**1. Bag of Words (BoW) or TF-IDF:**  \n",
    "- Good for simple models and small datasets.\n",
    "- Convert text to numerical vectors using BoW or TF-IDF.\n",
    "- Train a classifier (like Logistic Regression, SVM, or Naive Bayes) on these vectors.\n",
    "\n",
    "**2. Word Embeddings:**  \n",
    "- Use pre-trained embeddings (Word2Vec, GloVe, FastText) for better semantic understanding.\n",
    "- Average the word vectors or use them as input to a neural network.\n",
    "\n",
    "**3. Deep Learning / LLMs:**  \n",
    "- Use models like LSTM, GRU, or transformers (BERT, RoBERTa, DistilBERT).\n",
    "- These models capture context and word order, giving better results for complex data.\n",
    "\n",
    "**Recommendation:**  \n",
    "- For beginners or small projects, start with TF-IDF + Logistic Regression.\n",
    "- For higher accuracy, use a pre-trained transformer model (like BERT) with fine-tuning.\n",
    "\n",
    "**Example (TF-IDF + Logistic Regression):**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e3811a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Example data\n",
    "texts = [\"I love this product!\", \"This is terrible.\", \"Absolutely fantastic!\", \"Not good at all.\"]\n",
    "labels = [1, 0, 1, 0]  # 1 = positive, 0 = negative\n",
    "\n",
    "# TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Train classifier\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X, labels)\n",
    "\n",
    "# Predict sentiment\n",
    "test_texts = [\"I hate this\", \"Best ever!\"]\n",
    "X_test = vectorizer.transform(test_texts)\n",
    "predictions = clf.predict(X_test)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3f8acb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "For best results on large or complex datasets, use transformer-based models from Hugging Face Transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5516943a",
   "metadata": {},
   "source": [
    "____\n",
    "## Let's implement Bags of Word in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1944db66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba4d560a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "output",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "9f694362-e335-44ab-87c9-29e88e3c6e93",
       "rows": [
        [
         "0",
         "people watch dswithbappy",
         "1"
        ],
        [
         "1",
         "dswithbappy watch dswithbappy",
         "1"
        ],
        [
         "2",
         "people write comment",
         "0"
        ],
        [
         "3",
         "dswithbappy write comment",
         "0"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 4
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>people watch dswithbappy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dswithbappy watch dswithbappy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>people write comment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dswithbappy write comment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            text  output\n",
       "0       people watch dswithbappy       1\n",
       "1  dswithbappy watch dswithbappy       1\n",
       "2           people write comment       0\n",
       "3      dswithbappy write comment       0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\"text\":[\"people watch dswithbappy\",\n",
    "                         \"dswithbappy watch dswithbappy\",\n",
    "                         \"people write comment\",\n",
    "                          \"dswithbappy write comment\"],\"output\":[1,1,0,0]})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "879c096a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv= CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a63cd56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow=cv.fit_transform(df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9021da49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'people': 2, 'watch': 3, 'dswithbappy': 1, 'write': 4, 'comment': 0}\n"
     ]
    }
   ],
   "source": [
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae527d52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0],\n",
       "       [0, 2, 0, 1, 0],\n",
       "       [1, 0, 1, 0, 1],\n",
       "       [1, 1, 0, 0, 1]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "487b51fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0]]\n",
      "[[0 2 0 1 0]]\n",
      "[[1 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(bow[0].toarray())\n",
    "print(bow[1].toarray())\n",
    "print(bow[2].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c47c5bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 1, 0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new\n",
    "cv.transform(['Bappy watch dswithbappy']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4faebc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bow.toarray()\n",
    "y = df['output']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59c5171",
   "metadata": {},
   "source": [
    "### NGram\n",
    "\n",
    "means means if ngram value is 2 then it will consider 2 word as one token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0f3e200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "output",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "c096739d-72a9-49d6-a0d6-b02e1654a0e1",
       "rows": [
        [
         "0",
         "people watch dswithbappy",
         "1"
        ],
        [
         "1",
         "dswithbappy watch dswithbappy",
         "1"
        ],
        [
         "2",
         "people write comment",
         "0"
        ],
        [
         "3",
         "dswithbappy write comment",
         "0"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 4
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>people watch dswithbappy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dswithbappy watch dswithbappy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>people write comment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dswithbappy write comment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            text  output\n",
       "0       people watch dswithbappy       1\n",
       "1  dswithbappy watch dswithbappy       1\n",
       "2           people write comment       0\n",
       "3      dswithbappy write comment       0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\"text\":[\"people watch dswithbappy\",\n",
    "                         \"dswithbappy watch dswithbappy\",\n",
    "                         \"people write comment\",\n",
    "                          \"dswithbappy write comment\"],\"output\":[1,1,0,0]})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46d74803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BI grams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(ngram_range=(2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2338f4",
   "metadata": {},
   "source": [
    "This line creates an instance of the `CountVectorizer` class with the parameter `ngram_range=(2,2)`. The `CountVectorizer` is a tool from scikit-learn used to convert a collection of text documents into a matrix of token counts, which is a common first step in text analysis and machine learning workflows.\n",
    "\n",
    "By setting `ngram_range=(2,2)`, the vectorizer is configured to extract only bigrams (sequences of two consecutive words) from the input text, rather than the default unigrams (single words). For example, given the sentence \"machine learning is fun\", the bigrams extracted would be \"machine learning\", \"learning is\", and \"is fun\". When you later fit this vectorizer to a corpus, it will build a vocabulary of all unique bigrams found and represent each document as a vector indicating the count of each bigram.\n",
    "\n",
    "This approach is useful when you want to capture word pair relationships and context that single words alone might miss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fba24c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = cv.fit_transform(df['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6261b7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'people watch': 2, 'watch dswithbappy': 4, 'dswithbappy watch': 0, 'people write': 3, 'write comment': 5, 'dswithbappy write': 1}\n"
     ]
    }
   ],
   "source": [
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a60d81b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 1 0]]\n",
      "[[1 0 0 0 1 0]]\n",
      "[[0 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(bow[0].toarray())\n",
    "print(bow[1].toarray())\n",
    "print(bow[2].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ff91091",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ti gram\n",
    "# BI grams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(ngram_range=(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef3cc37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = cv.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3840ec0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'people watch dswithbappy': 2, 'dswithbappy watch dswithbappy': 0, 'people write comment': 3, 'dswithbappy write comment': 1}\n"
     ]
    }
   ],
   "source": [
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38df5e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0]]\n",
      "[[1 0 0 0]]\n",
      "[[0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(bow[0].toarray())\n",
    "print(bow[1].toarray())\n",
    "print(bow[2].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521a8dd4",
   "metadata": {},
   "source": [
    "____\n",
    "TF-IDF(Term frequency-Inverse document frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d124a9c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "output",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "7b330372-2e7e-41f5-aca0-63519fd0a92e",
       "rows": [
        [
         "0",
         "people watch dswithbappy",
         "1"
        ],
        [
         "1",
         "dswithbappy watch dswithbappy",
         "1"
        ],
        [
         "2",
         "people write comment",
         "0"
        ],
        [
         "3",
         "dswithbappy write comment",
         "0"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 4
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>people watch dswithbappy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dswithbappy watch dswithbappy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>people write comment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dswithbappy write comment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            text  output\n",
       "0       people watch dswithbappy       1\n",
       "1  dswithbappy watch dswithbappy       1\n",
       "2           people write comment       0\n",
       "3      dswithbappy write comment       0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\"text\":[\"people watch dswithbappy\",\n",
    "                         \"dswithbappy watch dswithbappy\",\n",
    "                         \"people write comment\",\n",
    "                          \"dswithbappy write comment\"],\"output\":[1,1,0,0]})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da61d31",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57f54229",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gensim\\__init__.py:11\u001b[39m\n\u001b[32m      7\u001b[39m __version__ = \u001b[33m'\u001b[39m\u001b[33m4.3.3\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[32m     14\u001b[39m logger = logging.getLogger(\u001b[33m'\u001b[39m\u001b[33mgensim\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logger.handlers:  \u001b[38;5;66;03m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gensim\\corpora\\__init__.py:6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mThis package contains implementations of various streaming corpus I/O format.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# bring corpus classes directly into package namespace, to save some typing\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mindexedcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IndexedCorpus  \u001b[38;5;66;03m# noqa:F401 must appear before the other classes\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmmcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MmCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbleicorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BleiCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gensim\\corpora\\indexedcorpus.py:14\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m interfaces, utils\n\u001b[32m     16\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIndexedCorpus\u001b[39;00m(interfaces.CorpusABC):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gensim\\interfaces.py:19\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[33;03m\"\"\"Basic interfaces used across the whole Gensim package.\u001b[39;00m\n\u001b[32m      8\u001b[39m \n\u001b[32m      9\u001b[39m \u001b[33;03mThese interfaces are used for building corpora, model transformation and similarity queries.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m \n\u001b[32m     15\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m utils, matutils\n\u001b[32m     22\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mCorpusABC\u001b[39;00m(utils.SaveLoad):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gensim\\matutils.py:1034\u001b[39m\n\u001b[32m   1029\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m1.\u001b[39m - \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mlen\u001b[39m(set1 & set2)) / \u001b[38;5;28mfloat\u001b[39m(union_cardinality)\n\u001b[32m   1032\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1033\u001b[39m     \u001b[38;5;66;03m# try to load fast, cythonized code if possible\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1034\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_matutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m logsumexp, mean_absolute_difference, dirichlet_expectation\n\u001b[32m   1036\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlogsumexp\u001b[39m(x):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gensim\\_matutils.pyx:1\u001b[39m, in \u001b[36minit gensim._matutils\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65d01b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
